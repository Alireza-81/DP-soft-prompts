{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DP-soft-prompts/Differentially-Private-Fine-tuning-of-Language-Models-main/Language-Generation-GPT-2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M49fSPouYUS0",
        "outputId": "8c0678d3-b37f-42ab-d12d-3e8ebcd9ae70"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#used these versions since they where used by the differential private finetuning code\n",
        "!pip install opacus==0.15.0\n",
        "!pip install torch==1.11.0\n",
        "!pip install datasets\n",
        "!pip install loralib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyT2tTO3-rrH",
        "outputId": "940a2038-811d-4275-8509-a7959629efb0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opacus==0.15.0 in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from opacus==0.15.0) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from opacus==0.15.0) (1.11.0)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.10/dist-packages (from opacus==0.15.0) (1.11.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->opacus==0.15.0) (4.5.0)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.10/dist-packages (1.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0) (4.5.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: loralib in /usr/local/lib/python3.10/dist-packages (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nqq3o2Me-b53"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup, get_scheduler, DataCollatorForLanguageModeling\n",
        "from opacus import PrivacyEngine\n",
        "from datasets import load_dataset\n",
        "#######################\n",
        "import os\n",
        "from os.path import join, abspath, dirname\n",
        "from data_utils.dataset import load_file, LAMADataset\n",
        "from data_utils.vocab import init_vocab\n",
        "from p_tune.modeling import PTuneForLAMA\n",
        "from transformers import AutoTokenizer\n",
        "#############################\n",
        "from loralib import MergedLinear\n",
        "import loralib as lora\n",
        "from opacus.grad_sample import utils as opacus_utils\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#functions used in this code\n",
        "def get_task_name():\n",
        "        if args.only_evaluate:\n",
        "            return \"_\".join([args.model_name + ('_' + args.vocab_strategy), 'only_evaluate'])\n",
        "        names = [args.model_name + ('_' + args.vocab_strategy),\n",
        "                 \"template_{}\".format(args.template if not args.use_original_template else 'original'),\n",
        "                 \"fixed\" if not args.use_lm_finetune else \"fine-tuned\",\n",
        "                 \"seed_{}\".format(args.seed)]\n",
        "        return \"_\".join(names)\n",
        "\n",
        "def get_TREx_parameters():\n",
        "        relation = load_file(join(args.data_dir, \"single_relations/{}.jsonl\".format(args.relation_id)))[0]\n",
        "        data_path_pre = \"fact-retrieval/original/{}/\".format(args.relation_id)\n",
        "        data_path_post = \".jsonl\"\n",
        "        return relation, data_path_pre, data_path_post\n",
        "def get_save_path():\n",
        "        return join(args.out_dir, 'prompt_model', args.model_name, 'search', get_task_name(),\n",
        "                    args.relation_id)\n",
        "\n",
        "def reverse_zero_pad(x, W, enable_lora, out_features):\n",
        "    lora_ind = W.new_zeros((out_features, ), dtype=torch.bool).view(len(enable_lora), -1)\n",
        "    lora_ind[enable_lora, :] = True\n",
        "    lora_ind = lora_ind.view(-1)\n",
        "    result = x.new_zeros((*x.shape[:-1], out_features // len(enable_lora) * sum(enable_lora)))\n",
        "    result = result.view(-1, out_features // len(enable_lora) * sum(enable_lora))\n",
        "    result = x.reshape(-1, out_features)[:, lora_ind]\n",
        "    return result.view((*x.shape[:-1], out_features // len(enable_lora) * sum(enable_lora)))\n",
        "\n",
        "\n",
        "def compute_transformers_MergedLinear_grad_sample(layer: MergedLinear, A: torch.Tensor, B: torch.Tensor, batch_dim: int = 0) -> None:\n",
        "    delta1 = reverse_zero_pad(B, layer.weight, layer.enable_lora, layer.out_features) * layer.scaling\n",
        "    after_A = F.linear(layer.lora_dropout(A), layer.lora_A)\n",
        "    t_after_A = after_A.transpose(-2, -1)\n",
        "    in_channel = t_after_A.shape[1]\n",
        "    out_channel = delta1.shape[-1]\n",
        "    lora_b_channel = layer.lora_B.shape[0]\n",
        "\n",
        "    gs1 = torch.einsum(\"nik,nkj->nij\", t_after_A[:, :in_channel//2, :], delta1[:, :, :out_channel//2])\n",
        "    gs2 = torch.einsum(\"nik,nkj->nij\", t_after_A[:, in_channel//2:, :], delta1[:, :, out_channel//2:])\n",
        "    opacus_utils.create_or_extend_grad_sample(layer.lora_B, torch.cat((gs1, gs2), -1).transpose(-2,-1).contiguous(), batch_dim)\n",
        "    gs3 = torch.einsum(\"nik,kj->nij\", delta1[:, :, :out_channel//2], layer.lora_B[:lora_b_channel//2, :])\n",
        "    gs4 = torch.einsum(\"nik,kj->nij\", delta1[:, :, out_channel//2:], layer.lora_B[lora_b_channel//2:, :])\n",
        "    after_A_deriv = torch.cat((gs3, gs4), -1)\n",
        "    lora_A_deriv = torch.einsum(\"nki,nkj->nij\", after_A_deriv, layer.lora_dropout(A))\n",
        "    opacus_utils.create_or_extend_grad_sample(layer.lora_A, lora_A_deriv.contiguous(), batch_dim)\n",
        "\n",
        "def evaluate(epoch_idx, evaluate_type):\n",
        "        model.eval()\n",
        "        if evaluate_type == 'Test':\n",
        "            loader = test_loader\n",
        "            dataset = test_set\n",
        "        else:\n",
        "            loader = dev_loader\n",
        "            dataset = dev_set\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            hit1, loss = 0, 0\n",
        "            for x_hs, x_ts in loader:\n",
        "                if False and self.args.extend_data:\n",
        "                    _loss, _hit1 = self.model.test_extend_data(x_hs, x_ts)\n",
        "                elif evaluate_type == 'Test':\n",
        "                    _loss, _hit1, top10 = model(x_hs, x_ts, return_candidates=True)\n",
        "                else:\n",
        "                    _loss, _hit1 = model(x_hs, x_ts)\n",
        "                hit1 += _hit1\n",
        "                loss += _loss.item()\n",
        "            hit1 /= len(dataset)\n",
        "            print(\"{} {} Epoch {} Loss: {} Hit@1:\".format(args.relation_id, evaluate_type, epoch_idx,\n",
        "                                                          loss / len(dataset)), hit1)\n",
        "        return loss, hit1"
      ],
      "metadata": {
        "id": "K6CrsdpnY7-C"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    init_checkpoint = \"/content/drive/MyDrive/DP-soft-prompts/Differentially-Private-Fine-tuning-of-Language-Models-main/Language-Generation-GPT-2/pretrained_checkpoints/gpt2-pytorch_model.bin\"\n",
        "    learning_rate = 2e-5\n",
        "    train_batch_size = 8\n",
        "    grad_acc_steps = 1\n",
        "    epochs = 3\n",
        "    noise_multiplier = 1.0\n",
        "    max_grad_norm = 1.0\n",
        "    lstm_dropout = 0.1\n",
        "    hidden_size = 768\n",
        "    max_length = 128\n",
        "\n",
        "    relation_id = \"P1001\"\n",
        "    model_name = 'gpt2'\n",
        "    pseudo_token = '[PROMPT]'\n",
        "\n",
        "    t5_shard = 0\n",
        "    mid = 0\n",
        "    template = (3, 3, 3)\n",
        "    early_stop = 20\n",
        "\n",
        "    lr = 1e-5\n",
        "    seed = 34\n",
        "    decay_rate = 0.98\n",
        "    weight_decay = 0.0005\n",
        "    no_cuda = False\n",
        "    seq_len = 512\n",
        "\n",
        "    train_data = \"/content/drive/MyDrive/DP-soft-prompts/Differentially-Private-Fine-tuning-of-Language-Models-main/Language-Generation-GPT-2/data/e2e/train.jsonl\"\n",
        "    valid_data = \"/content/drive/MyDrive/DP-soft-prompts/Differentially-Private-Fine-tuning-of-Language-Models-main/Language-Generation-GPT-2/data/e2e/valid.jsonl\"\n",
        "\n",
        "\n",
        "    only_evaluate = False\n",
        "    use_original_template = False\n",
        "    use_lm_finetune = False\n",
        "\n",
        "    vocab_strategy = \"shared\"\n",
        "\n",
        "    # directories\n",
        "    data_dir = '/content/drive/MyDrive/DP-soft-prompts/Differentially-Private-Fine-tuning-of-Language-Models-main/Language-Generation-GPT-2/data/LAMA'\n",
        "    out_dir = '/content/drive/MyDrive/DP-soft-prompts/Differentially-Private-Fine-tuning-of-Language-Models-main/Language-Generation-GPT-2/out/LAMA'\n",
        "\n",
        "\n",
        "    lora_dim = 4\n",
        "    lora_alpha = 32\n",
        "    lora_dropout = 0.0\n",
        "    label_smooth = 0.1\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    n_gpu = 0 if no_cuda else torch.cuda.device_count()\n",
        "\n",
        "    assert isinstance(template, tuple)\n",
        "\n",
        "args = Args()"
      ],
      "metadata": {
        "id": "oGvNDM-cgOFg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer init\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=False)\n",
        "relation, data_path_pre, data_path_post = get_TREx_parameters()\n",
        "init_vocab(args)\n",
        "#data processing\n",
        "train_data = load_file(join(args.data_dir, data_path_pre + 'train' + data_path_post))\n",
        "dev_data = load_file(join(args.data_dir, data_path_pre + 'dev' + data_path_post))\n",
        "test_data = load_file(join(args.data_dir, data_path_pre + 'test' + data_path_post))\n",
        "\n",
        "test_set = LAMADataset('test', test_data, tokenizer, args)\n",
        "train_set = LAMADataset('train', train_data, tokenizer, args)\n",
        "dev_set = LAMADataset('dev', dev_data, tokenizer, args)\n",
        "os.makedirs(get_save_path(), exist_ok=True)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=8, shuffle=True, drop_last=True)\n",
        "dev_loader = DataLoader(dev_set, batch_size=8)\n",
        "test_loader = DataLoader(test_set, batch_size=8)\n"
      ],
      "metadata": {
        "id": "hiNsBRUJYxVS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PTuneForLAMA(args, args.device, args.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEoUdu2_RQ-u",
        "outputId": "33333350-72db-429f-984c-ab7c2f2e946d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading model pretrained weight.\n",
            "init prompt encoder...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if args.lora_dim > 0:\n",
        "      lora.mark_only_lora_as_trainable(model.model)\n",
        "opacus_utils.register_grad_sampler(MergedLinear)(compute_transformers_MergedLinear_grad_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8e2rvWYVSQ7",
        "outputId": "f689ca8e-6077-466e-e181-5625c318e73c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.compute_transformers_MergedLinear_grad_sample(layer: loralib.layers.MergedLinear, A: torch.Tensor, B: torch.Tensor, batch_dim: int = 0) -> None>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_to_optimize = []\n",
        "for name, param in model.named_parameters():\n",
        "    if name == \"prompt_encoder.embedding.weight\":\n",
        "            param.requires_grad = False\n",
        "    if param.requires_grad:\n",
        "            params_to_optimize.append({'params': param})\n",
        "            #print(name)"
      ],
      "metadata": {
        "id": "LckDzgfxUZnJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params_to_optimize, lr=args.lr, weight_decay=args.weight_decay)"
      ],
      "metadata": {
        "id": "h3EYlW1fYHGi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#attaching the privacy engine\n",
        "ALPHAS = [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64))\n",
        "SAMPLE_RATE = (args.train_batch_size * args.grad_acc_steps)/42061.0\n",
        "privacy_engine = PrivacyEngine(\n",
        "    module=model,\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    alphas=ALPHAS,\n",
        "    noise_multiplier=args.noise_multiplier,\n",
        "    max_grad_norm=args.max_grad_norm,\n",
        ")\n",
        "privacy_engine.attach(optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAXmGLm-Z_pq",
        "outputId": "6bf8686a-2650-4b1f-937a-6d08846ae2d5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:759: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:236: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check if privacy engine is attached - for debug\n",
        "if hasattr(optimizer, \"privacy_engine\"):\n",
        "    print(\"PrivacyEngine is attached.\")\n",
        "    is_attached = isinstance(optimizer.privacy_engine, PrivacyEngine)\n",
        "    print(f\"PrivacyEngine is correctly attached: {is_attached}\")\n",
        "else:\n",
        "    print(\"PrivacyEngine is NOT attached.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVzZb4b2leMO",
        "outputId": "b4f7d557-57be-4f1e-b889-333af16a700c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PrivacyEngine is NOT attached.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop\n",
        "best_dev, early_stop, has_adjusted = 0, 0, True\n",
        "for epoch_idx in range(100):\n",
        "    if epoch_idx > -1:\n",
        "        dev_loss, dev_hit1 = evaluate(epoch_idx, 'Dev')\n",
        "        if epoch_idx == 0:\n",
        "            test_loss, test_hit1 = evaluate(epoch_idx, 'Test')\n",
        "        if epoch_idx > 0 and (dev_hit1 >= best_dev) or args.only_evaluate:\n",
        "            test_loss, test_hit1 = evaluate(epoch_idx, 'Test')\n",
        "            #best_ckpt = self.get_checkpoint(epoch_idx, dev_hit1, test_hit1)\n",
        "            early_stop = 0\n",
        "            best_dev = dev_hit1\n",
        "        else:\n",
        "            early_stop += 1\n",
        "            if early_stop >= args.early_stop:\n",
        "                #self.save(best_ckpt)\n",
        "                print(\"{} Early stopping at epoch {}.\".format(args.relation_id, epoch_idx))\n",
        "                break\n",
        "    if args.only_evaluate:\n",
        "        break\n",
        "\n",
        "        # run training\n",
        "    hit1, num_of_samples = 0, 0\n",
        "    tot_loss = 0\n",
        "    for batch_idx, batch in tqdm(enumerate(train_loader)):\n",
        "        optimizer.zero_grad()\n",
        "        model.train()\n",
        "        loss, batch_hit1 = model(batch[0], batch[1])\n",
        "        hit1 += batch_hit1\n",
        "        tot_loss += loss.item()\n",
        "        num_of_samples += len(batch[0])\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        optimizer.step()\n",
        "        torch.cuda.empty_cache()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "2uXoF4FD4Ozi",
        "outputId": "d965e4fa-d45e-4b6e-dfe1-e0197b5bb588"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P1001 Dev Epoch 0 Loss: 0.16870941986908783 Hit@1: 0.6702702702702703\n",
            "P1001 Test Epoch 0 Loss: 0.10551043590867376 Hit@1: 0.8102409638554217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 4.44 GiB (GPU 0; 14.75 GiB total capacity; 9.31 GiB already allocated; 4.44 GiB free; 9.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-fe6c17c8450d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opacus/grad_sample/grad_sample_module.py\u001b[0m in \u001b[0;36mcapture_backprops_hook\u001b[0;34m(self, module, _forward_input, forward_output, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    207\u001b[0m         )\n\u001b[1;32m    208\u001b[0m         \u001b[0mgrad_sampler_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRAD_SAMPLERS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mgrad_sampler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opacus/grad_sample/linear.py\u001b[0m in \u001b[0;36mcompute_linear_grad_sample\u001b[0;34m(layer, A, B, batch_dim)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \"\"\"\n\u001b[1;32m     23\u001b[0m     \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n...i,n...j->nij\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mcreate_or_extend_grad_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opacus/grad_sample/utils.py\u001b[0m in \u001b[0;36mcreate_or_extend_grad_sample\u001b[0;34m(param, grad_sample, batch_dim)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"grad_sample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 4.44 GiB (GPU 0; 14.75 GiB total capacity; 9.31 GiB already allocated; 4.44 GiB free; 9.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    }
  ]
}