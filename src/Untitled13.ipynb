{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GkmUGaOtyGVX","executionInfo":{"status":"ok","timestamp":1704546631478,"user_tz":-210,"elapsed":4925,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}},"outputId":"c94620ad-f532-48ad-cb53-2cd1711812c2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Rlv2ZTo0g62K","executionInfo":{"status":"ok","timestamp":1704546631478,"user_tz":-210,"elapsed":10,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}}},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/DP-soft-prompts/Differentially-Private-Fine-tuning-of-Language-Models-main/Language-Generation-GPT-2')"]},{"cell_type":"code","source":["!pip install progress"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0-k7XcOcwdp3","executionInfo":{"status":"ok","timestamp":1704546636423,"user_tz":-210,"elapsed":4954,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}},"outputId":"d90a3755-6f35-4856-e0bd-5d095a602093"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: progress in /usr/local/lib/python3.10/dist-packages (1.6)\n"]}]},{"cell_type":"code","source":["!pip install loralib"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4e3JOkZ0KNY","executionInfo":{"status":"ok","timestamp":1704546640522,"user_tz":-210,"elapsed":4103,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}},"outputId":"39db2341-75a2-40f0-f1e0-9b2d66f38492"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: loralib in /usr/local/lib/python3.10/dist-packages (0.1.2)\n"]}]},{"cell_type":"code","source":["!pip install torch==1.11.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ACK319zDnaxY","executionInfo":{"status":"ok","timestamp":1704546644656,"user_tz":-210,"elapsed":4138,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}},"outputId":"78a7749e-ac70-4f29-9700-8de71e1a2697"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.10/dist-packages (1.11.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0) (4.5.0)\n"]}]},{"cell_type":"code","source":["!pip install LAMA"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zdMYGpD93elx","executionInfo":{"status":"ok","timestamp":1704546649907,"user_tz":-210,"elapsed":5254,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}},"outputId":"c2d35c14-29c6-470b-fede-0887a4cfac63"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: LAMA in /usr/local/lib/python3.10/dist-packages (0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from LAMA) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->LAMA) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->LAMA) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->LAMA) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->LAMA) (2023.11.17)\n"]}]},{"cell_type":"code","source":["from src.model import GPT2Config, GPT2LMModel\n","from p_tune.modeling import PTuneForLAMA\n","from data_utils.dataset import load_file, LAMADataset\n","from data_utils.vocab import init_vocab"],"metadata":{"id":"c7b1sGKkz-Ro","executionInfo":{"status":"ok","timestamp":1704546651088,"user_tz":-210,"elapsed":1185,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["!pip install opacus==0.15.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vVT1bHAJ7PQW","executionInfo":{"status":"ok","timestamp":1704546655390,"user_tz":-210,"elapsed":4306,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}},"outputId":"894a1674-b807-4673-9877-a9cfa47f1ed8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opacus==0.15.0 in /usr/local/lib/python3.10/dist-packages (0.15.0)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from opacus==0.15.0) (1.23.5)\n","Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from opacus==0.15.0) (1.11.0)\n","Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.10/dist-packages (from opacus==0.15.0) (1.11.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->opacus==0.15.0) (4.5.0)\n"]}]},{"cell_type":"code","source":["import argparse\n","import time\n","import math\n","import os, sys\n","import warnings\n","\n","import numpy as np\n","import itertools\n","\n","import torch\n","import torch.nn.functional as F\n","import random\n","from torch.utils.data import DataLoader\n","torch.set_printoptions(threshold=100000)\n","from opacus import PrivacyEngine\n","from opacus.grad_sample import utils as opacus_utils\n","from opacus.layers import DifferentiallyPrivateDistributedDataParallel as DPDDP"],"metadata":{"id":"lBSczFV_7Ifq","executionInfo":{"status":"ok","timestamp":1704546656157,"user_tz":-210,"elapsed":771,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from src.gpu import (\n","    add_gpu_params,\n","    parse_gpu,\n","    distributed_opt,\n","    distributed_gather,\n","    distributed_sync,\n","    cleanup\n",")\n","from src.optimizer import (\n","    create_adam_optimizer,\n","    create_optimizer_scheduler,\n","    add_optimizer_params,\n","    create_adam_optimizer_from_args\n",")\n","\n","from src.data_utils import FT_Dataset\n","from src.model import GPT2Config, GPT2LMModel\n","from src.exp_utils import create_exp_dir\n","\n","from loralib import MergedLinear\n","import loralib as lora\n","\n","parser = argparse.ArgumentParser(description='PyTorch GPT2 ft script')\n","\n","add_gpu_params(parser)\n","add_optimizer_params(parser)\n","device = torch.device('cuda')\n","\n","\n","class Args:\n","    train_data = '/content/drive/MyDrive/DP-soft-prompts/Differentially-Private-Fine-tuning-of-Language-Models-main/Language-Generation-GPT-2/data/e2e/train.jsonl'  # You need to set this to your actual data path\n","    valid_data = '/content/drive/MyDrive/DP-soft-prompts/Differentially-Private-Fine-tuning-of-Language-Models-main/Language-Generation-GPT-2/data/e2e/valid.jsonl'  # You need to set this to your actual data path\n","    train_batch_size = 8\n","    valid_batch_size = 4\n","    grad_acc = 1\n","    clip = 0.0\n","    noise_multiplier = 0.5\n","    max_grad_norm = 1.0\n","    seq_len = 512\n","    model_card = 'gpt2.sm'\n","    init_checkpoint = '/content/drive/MyDrive/DP-soft-prompts/Differentially-Private-Fine-tuning-of-Language-Models-main/Language-Generation-GPT-2/pretrained_checkpoints/gpt2-pytorch_model.bin'\n","    fp16 = False\n","    log_interval = 100\n","    eval_interval = 2000\n","    save_interval = 500\n","    work_dir = os.getenv('PT_OUTPUT_DIR', 'gpt2_model')\n","    lora_dim = 0\n","    lora_alpha = 128\n","    obj = 'clm'\n","    lora_dropout = 0.0\n","    label_smooth = 0.0\n","    roll_interval = -1\n","    roll_lr = 0.00001\n","    roll_step = 100\n","    eval_epoch = 1\n","    random_seed = 42\n","\n","    lr = 0.00001\n","    weight_decay = 0.01\n","    correct_bias = False  # default for 'store_true' is False\n","    adam_epislon = 1e-6\n","    no_decay_bias = False  # default for 'store_true' is False, but we keep the original value\n","    adam_beta1 = 0.9\n","    adam_beta2 = 0.98\n","    scheduler = 'linear'\n","    max_step = None\n","    max_epoch = 5\n","    warmup_step = 0\n","    i_steps = '0'\n","    i_lrs = '0.00025'\n","    platform = 'local'\n","    world_size = 1\n","    device = device\n","    rank = 0\n","\n","args = Args()\n","\n","\n","class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\n","         Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n","    \"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def optimizer_step(_loss, _optimizer, _model, _schedule, args, is_update=True):\n","    #if args.fp16:\n","    #    with amp.scale_loss(_loss, _optimizer) as _scaled_loss:\n","    #        _scaled_loss.backward()\n","    #else:\n","    _loss.backward()\n","\n","    if is_update:\n","        if args.clip > 0:\n","            if args.fp16:\n","                torch.nn.utils.clip_grad_norm_(amp.master_params(_optimizer), args.clip)\n","            else:\n","                torch.nn.utils.clip_grad_norm_(_model.parameters(), args.clip)\n","\n","        _optimizer.step()\n","        _optimizer.zero_grad()\n","    else:\n","        # This should fail if we are using any gradient accumulation with Opacus DDP\n","        _optimizer.virtual_step()\n","\n","    if _schedule is not None:\n","        _schedule.step()\n","\n","\n","def evaluate(model, valid_loader, args):\n","    model.eval()\n","    total_loss = 0.\n","    start_time = time.time()\n","\n","    avg_lm_loss = AverageMeter()\n","\n","    with torch.no_grad():\n","        for idx, data in enumerate(valid_loader):\n","            data = {key: value for key, value in data.items()}\n","\n","            _input = data['input'].to(args.device)\n","            _target = data['target'].to(args.device)\n","            _msk = data['mask'].to(args.device)\n","\n","            _lm_logits, _loss = model(_input, lm_labels=_target, lm_mask=_msk)\n","            loss = _loss.mean()\n","\n","            avg_lm_loss.update(loss.item())\n","\n","            if idx % 100 == 0:\n","                print('eval samples:', idx, 'loss:', loss.float())\n","\n","        total_time = time.time() - start_time\n","        print('average loss', avg_lm_loss.avg)\n","    return avg_lm_loss.avg, math.exp(avg_lm_loss.avg)\n","\n","def train_validate(\n","    model,\n","    optimizer,\n","    scheduler,\n","    train_loader,\n","    valid_loader,\n","    args,\n","    train_step=0,\n","    epoch=0\n","):\n","    model.train()\n","    avg_lm_loss = AverageMeter()\n","    print('start to train the model................', epoch)\n","    log_start_time = time.time()\n","    best_val_ppl = None\n","\n","    #train_loader.sampler.set_epoch(epoch)\n","\n","\n","    for idx, data in enumerate(train_loader):\n","        data = {key: value for key, value in data.items()}\n","\n","        _input = data['input'].to(args.device)\n","        _target = data['target'].to(args.device)\n","        _msk = data['mask'].to(args.device)\n","\n","        _lm_logits, _lm_loss = model(\n","            _input, lm_labels=_target, lm_mask=_msk, label_smooth=args.label_smooth\n","        )\n","\n","        _lm_loss = _lm_loss.mean()\n","\n","        train_step += 1\n","        is_update = True if train_step % args.grad_acc == 0 else False\n","        avg_lm_loss.update(_lm_loss.item())\n","        # optimizer_step(\n","        #     _lm_loss/(args.grad_acc), optimizer, model, scheduler, args, is_update=is_update\n","        # )\n","        optimizer_step(\n","            _lm_loss, optimizer, model, scheduler, args, is_update=is_update\n","        )\n","\n","        if train_step % args.log_interval == 0:\n","            elapsed = time.time() - log_start_time\n","            lr = optimizer.param_groups[0]['lr']\n","            log_str = f'| epoch {epoch:3d} step {train_step:>8d} | { idx + 1:>6d} batches | ' \\\n","                      f'lr {lr:.3g} | ms/batch {elapsed * 1000 / args.log_interval:5.2f} | ' \\\n","                      f'loss {avg_lm_loss.val:5.2f} | avg loss {avg_lm_loss.avg:5.2f} | ' \\\n","                      f'ppl {math.exp(avg_lm_loss.avg):5.2f}'\n","\n","            if args.rank == 0:\n","                print(log_str)\n","            log_start_time = time.time()\n","            avg_lm_loss.reset()\n","\n","        if train_step % args.save_interval == 0:\n","            if args.rank == 0:\n","                model_path = os.path.join(args.work_dir, f'model.{train_step}.pt')\n","                print('saving checkpoint', model_path)\n","                torch.save({'model_state_dict': lora.lora_state_dict(model)}, model_path)\n","            distributed_sync(args)\n","\n","        # evaluation interval\n","        if train_step % args.eval_interval == 0:\n","            eval_start_time = time.time()\n","\n","            valid_loss, valid_ppl = evaluate(model, valid_loader, args)\n","\n","            if best_val_ppl is None or valid_ppl < best_val_ppl:\n","                best_val_ppl = valid_ppl\n","\n","            log_str = f'| Eval {train_step // args.eval_interval:3d} at step {train_step:>8d} | ' \\\n","                      f'time: {time.time() - eval_start_time:5.2f}s | valid loss {valid_loss:5.2f} | ' \\\n","                      f'valid ppl {valid_ppl:5.2f} | best ppl {best_val_ppl:5.2f} '\n","\n","            if args.rank == 0:\n","                print('-' * 100)\n","                print(log_str)\n","                print('-' * 100)\n","\n","            model.train()\n","            distributed_sync(args)\n","\n","        if train_step == args.max_step:\n","            break\n","\n","    if args.rank == 0:\n","        model_path = os.path.join(args.work_dir, f'model.{train_step}.pt')\n","        print('saving checkpoint', model_path)\n","        torch.save({'model_state_dict': model.state_dict()}, model_path)\n","    distributed_sync(args)\n","    return train_step"],"metadata":{"id":"RZonyYJa7bbl","executionInfo":{"status":"ok","timestamp":1704547890342,"user_tz":-210,"elapsed":4,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["def reverse_zero_pad(x, W, enable_lora, out_features):\n","    lora_ind = W.new_zeros((out_features, ), dtype=torch.bool).view(len(enable_lora), -1)\n","    lora_ind[enable_lora, :] = True\n","    lora_ind = lora_ind.view(-1)\n","    result = x.new_zeros((*x.shape[:-1], out_features // len(enable_lora) * sum(enable_lora)))\n","    result = result.view(-1, out_features // len(enable_lora) * sum(enable_lora))\n","    result = x.reshape(-1, out_features)[:, lora_ind]\n","    return result.view((*x.shape[:-1], out_features // len(enable_lora) * sum(enable_lora)))\n","\n","\n","def compute_transformers_MergedLinear_grad_sample(layer: MergedLinear, A: torch.Tensor, B: torch.Tensor, batch_dim: int = 0) -> None:\n","    delta1 = reverse_zero_pad(B, layer.weight, layer.enable_lora, layer.out_features) * (layer.lora_alpha / layer.r)\n","    after_A = F.linear(layer.lora_dropout(A), layer.lora_A)\n","    t_after_A = after_A.transpose(-2, -1)\n","    in_channel = t_after_A.shape[1]\n","    out_channel = delta1.shape[-1]\n","    lora_b_channel = layer.lora_B.shape[0]\n","\n","    gs1 = torch.einsum(\"nik,nkj->nij\", t_after_A[:, :in_channel//2, :], delta1[:, :, :out_channel//2])\n","    gs2 = torch.einsum(\"nik,nkj->nij\", t_after_A[:, in_channel//2:, :], delta1[:, :, out_channel//2:])\n","    opacus_utils.create_or_extend_grad_sample(layer.lora_B, torch.cat((gs1, gs2), -1).transpose(-2,-1).contiguous(), batch_dim)\n","    gs3 = torch.einsum(\"nik,kj->nij\", delta1[:, :, :out_channel//2], layer.lora_B[:lora_b_channel//2, :])\n","    gs4 = torch.einsum(\"nik,kj->nij\", delta1[:, :, out_channel//2:], layer.lora_B[lora_b_channel//2:, :])\n","    after_A_deriv = torch.cat((gs3, gs4), -1)\n","    lora_A_deriv = torch.einsum(\"nki,nkj->nij\", after_A_deriv, layer.lora_dropout(A))\n","    opacus_utils.create_or_extend_grad_sample(layer.lora_A, lora_A_deriv.contiguous(), batch_dim)"],"metadata":{"id":"uLUctz1XGeRK","executionInfo":{"status":"ok","timestamp":1704547894011,"user_tz":-210,"elapsed":541,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["    torch.manual_seed(args.random_seed)\n","    random.seed(args.random_seed)\n","\n","\n","    train_data = FT_Dataset(\n","        args.train_data, args.train_batch_size, args.seq_len,\n","        joint_lm=args.obj=='jlm'\n","    )\n","\n","    valid_data = FT_Dataset(\n","        args.valid_data, args.valid_batch_size, args.seq_len,\n","    )\n","\n","    train_loader = DataLoader(\n","        train_data, batch_size=args.train_batch_size, num_workers=0,\n","        shuffle=True, pin_memory=False, drop_last=True\n","    )\n","\n","    valid_loader = DataLoader(\n","        valid_data, batch_size=args.valid_batch_size, num_workers=0,\n","        shuffle=False, pin_memory=False, drop_last=False\n","    )\n","\n","    if args.model_card == 'gpt2.sm':\n","        config = GPT2Config(\n","            n_embd=768, n_layer=12, n_head=12,\n","            lora_attn_dim=args.lora_dim,\n","            lora_attn_alpha=args.lora_alpha,\n","            lora_dropout=args.lora_dropout,\n","        )\n","    elif args.model_card == 'gpt2.md':\n","        config = GPT2Config(\n","            n_embd=1024, n_layer=24, n_head=16,\n","            lora_attn_dim=args.lora_dim,\n","            lora_attn_alpha=args.lora_alpha,\n","            lora_dropout=args.lora_dropout,\n","        )\n","\n",""],"metadata":{"id":"AoShlfKh0W-B","executionInfo":{"status":"ok","timestamp":1704547775881,"user_tz":-210,"elapsed":959,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":[" ###################\n","lm_net = GPT2LMModel(config)\n","#lm_net = SoftPromptTuning(lm_net, prompt_length)\n","if args.init_checkpoint is not None:\n","  print('loading model pretrained weight.')\n","  lm_net.load_weight(torch.load(args.init_checkpoint))\n","\n","#for param in lm_net.pretrained_model.parameters():\n","#    param.requires_grad = False\n","#for param in lm_net.prompt_embeddings.parameters():\n","#    param.requires_grad = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HaTmfMKT53W5","executionInfo":{"status":"ok","timestamp":1704547781293,"user_tz":-210,"elapsed":3094,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}},"outputId":"74509fb1-dcaf-43fc-df9f-8a935c59567d"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["loading model pretrained weight.\n"]}]},{"cell_type":"code","source":["lm_net = lm_net.cuda()\n","if args.lora_dim > 0:\n","    lora.mark_only_lora_as_trainable(lm_net)\n","opacus_utils.register_grad_sampler(MergedLinear)(compute_transformers_MergedLinear_grad_sample)\n","#lm_net = DPDDP(lm_net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQZ4L96q6I6R","executionInfo":{"status":"ok","timestamp":1704547781294,"user_tz":-210,"elapsed":7,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}},"outputId":"f9130a35-37b4-4d92-fbbf-6e1fa134534f"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.compute_transformers_MergedLinear_grad_sample(layer: loralib.layers.MergedLinear, A: torch.Tensor, B: torch.Tensor, batch_dim: int = 0) -> None>"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["############################\n","optimizer = create_adam_optimizer_from_args(lm_net, args)\n"],"metadata":{"id":"jwNCQ1gR6Kos","executionInfo":{"status":"ok","timestamp":1704547784491,"user_tz":-210,"elapsed":446,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["if args.max_step is None:\n","\n","    args.max_step = (args.max_epoch * train_data.num_batches + args.world_size - 1) // args.world_size\n","    print('set max_step:', args.max_step)\n","\n","scheduler = create_optimizer_scheduler(optimizer, args)\n","if args.fp16:\n","    lm_net, optimizer = amp.initialize(lm_net, optimizer, opt_level=\"O1\")\n","\n","n_layers = len([(n, p) for n, p in lm_net.named_parameters() if p.requires_grad])\n","max_grad_norm = [args.max_grad_norm / np.sqrt(n_layers)] * n_layers\n","\n","ALPHAS = [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttu2EZOP6dCZ","executionInfo":{"status":"ok","timestamp":1704547786312,"user_tz":-210,"elapsed":5,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}},"outputId":"e981c87f-21e1-45c1-c34d-17aa0f1b6db6"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["set max_step: 52580\n"]}]},{"cell_type":"code","source":["##################################\n","#num_trainable_params = sum(p.numel() for p in lm_net.prompt_embeddings.parameters() if p.requires_grad)\n","ALPHAS = [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64))\n","    # We instead use the accountant from Gopi et al. (2021) as described in the paper.\n","SAMPLE_RATE = (args.train_batch_size * args.grad_acc)/42061.0\n","privacy_engine = PrivacyEngine(\n","    module=lm_net,\n","    sample_rate=SAMPLE_RATE,\n","    alphas=ALPHAS,\n","    noise_multiplier=args.noise_multiplier,\n","    max_grad_norm=max_grad_norm,\n",")\n","privacy_engine.attach(optimizer)\n","\n","\n","delta = 1.0/42061 # We instead use the accountant from Gopi et al. (2021) as described in the paper.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":762},"id":"s0ULS9lA6dvu","executionInfo":{"status":"error","timestamp":1704547788666,"user_tz":-210,"elapsed":590,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}},"outputId":"ca8eb943-14cd-44ed-aa0c-cc59c22d5486"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:759: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:236: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n"]},{"output_type":"error","ename":"IncompatibleModuleException","evalue":"Model contains incompatible modules.\ngrad_sampler method is not yet supported for this module.: ['_module.transformer.h.0.ln_1 (LayerNorm)', '_module.transformer.h.0.attn.c_proj (Conv1D)', '_module.transformer.h.0.ln_2 (LayerNorm)', '_module.transformer.h.0.mlp.c_fc (Conv1D)', '_module.transformer.h.0.mlp.c_proj (Conv1D)', '_module.transformer.h.1.ln_1 (LayerNorm)', '_module.transformer.h.1.attn.c_proj (Conv1D)', '_module.transformer.h.1.ln_2 (LayerNorm)', '_module.transformer.h.1.mlp.c_fc (Conv1D)', '_module.transformer.h.1.mlp.c_proj (Conv1D)', '_module.transformer.h.2.ln_1 (LayerNorm)', '_module.transformer.h.2.attn.c_proj (Conv1D)', '_module.transformer.h.2.ln_2 (LayerNorm)', '_module.transformer.h.2.mlp.c_fc (Conv1D)', '_module.transformer.h.2.mlp.c_proj (Conv1D)', '_module.transformer.h.3.ln_1 (LayerNorm)', '_module.transformer.h.3.attn.c_proj (Conv1D)', '_module.transformer.h.3.ln_2 (LayerNorm)', '_module.transformer.h.3.mlp.c_fc (Conv1D)', '_module.transformer.h.3.mlp.c_proj (Conv1D)', '_module.transformer.h.4.ln_1 (LayerNorm)', '_module.transformer.h.4.attn.c_proj (Conv1D)', '_module.transformer.h.4.ln_2 (LayerNorm)', '_module.transformer.h.4.mlp.c_fc (Conv1D)', '_module.transformer.h.4.mlp.c_proj (Conv1D)', '_module.transformer.h.5.ln_1 (LayerNorm)', '_module.transformer.h.5.attn.c_proj (Conv1D)', '_module.transformer.h.5.ln_2 (LayerNorm)', '_module.transformer.h.5.mlp.c_fc (Conv1D)', '_module.transformer.h.5.mlp.c_proj (Conv1D)', '_module.transformer.h.6.ln_1 (LayerNorm)', '_module.transformer.h.6.attn.c_proj (Conv1D)', '_module.transformer.h.6.ln_2 (LayerNorm)', '_module.transformer.h.6.mlp.c_fc (Conv1D)', '_module.transformer.h.6.mlp.c_proj (Conv1D)', '_module.transformer.h.7.ln_1 (LayerNorm)', '_module.transformer.h.7.attn.c_proj (Conv1D)', '_module.transformer.h.7.ln_2 (LayerNorm)', '_module.transformer.h.7.mlp.c_fc (Conv1D)', '_module.transformer.h.7.mlp.c_proj (Conv1D)', '_module.transformer.h.8.ln_1 (LayerNorm)', '_module.transformer.h.8.attn.c_proj (Conv1D)', '_module.transformer.h.8.ln_2 (LayerNorm)', '_module.transformer.h.8.mlp.c_fc (Conv1D)', '_module.transformer.h.8.mlp.c_proj (Conv1D)', '_module.transformer.h.9.ln_1 (LayerNorm)', '_module.transformer.h.9.attn.c_proj (Conv1D)', '_module.transformer.h.9.ln_2 (LayerNorm)', '_module.transformer.h.9.mlp.c_fc (Conv1D)', '_module.transformer.h.9.mlp.c_proj (Conv1D)', '_module.transformer.h.10.ln_1 (LayerNorm)', '_module.transformer.h.10.attn.c_proj (Conv1D)', '_module.transformer.h.10.ln_2 (LayerNorm)', '_module.transformer.h.10.mlp.c_fc (Conv1D)', '_module.transformer.h.10.mlp.c_proj (Conv1D)', '_module.transformer.h.11.ln_1 (LayerNorm)', '_module.transformer.h.11.attn.c_proj (Conv1D)', '_module.transformer.h.11.ln_2 (LayerNorm)', '_module.transformer.h.11.mlp.c_fc (Conv1D)', '_module.transformer.h.11.mlp.c_proj (Conv1D)', '_module.transformer.ln_f (LayerNorm)']","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIncompatibleModuleException\u001b[0m               Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-f7c3ccd2210a>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprivacy_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py\u001b[0m in \u001b[0;36mattach\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         norm_clipper = (\n\u001b[1;32m    309\u001b[0m             \u001b[0mclipping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConstantFlatClipper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opacus/dp_model_inspector.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minspector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviolators\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\n{inspector.message}: {inspector.violators}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIncompatibleModuleException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIncompatibleModuleException\u001b[0m: Model contains incompatible modules.\ngrad_sampler method is not yet supported for this module.: ['_module.transformer.h.0.ln_1 (LayerNorm)', '_module.transformer.h.0.attn.c_proj (Conv1D)', '_module.transformer.h.0.ln_2 (LayerNorm)', '_module.transformer.h.0.mlp.c_fc (Conv1D)', '_module.transformer.h.0.mlp.c_proj (Conv1D)', '_module.transformer.h.1.ln_1 (LayerNorm)', '_module.transformer.h.1.attn.c_proj (Conv1D)', '_module.transformer.h.1.ln_2 (LayerNorm)', '_module.transformer.h.1.mlp.c_fc (Conv1D)', '_module.transformer.h.1.mlp.c_proj (Conv1D)', '_module.transformer.h.2.ln_1 (LayerNorm)', '_module.transformer.h.2.attn.c_proj (Conv1D)', '_module.transformer.h.2.ln_2 (LayerNorm)', '_module.transformer.h.2.mlp.c_fc (Conv1D)', '_module.transformer.h.2.mlp.c_proj (Conv1D)', '_module.transformer.h.3.ln_1 (LayerNorm)', '_module.transformer.h.3.attn.c_proj (Conv1D)', '_module.transformer.h.3.ln_2 (LayerNorm)', '_module.transformer.h.3.mlp.c_fc (Conv1D)', '_module.transformer.h.3.mlp.c_proj (Conv1D)', '_module.transformer.h.4.ln_1 (LayerNorm)', '_module.transformer.h.4.attn.c_proj (Conv1D)', '_module.transformer.h.4.ln_2 (LayerNorm)', '_module.transformer.h.4.mlp.c_fc (Conv1D)', '_module.transformer.h.4.mlp.c_proj (Conv1D)', '_module.transformer.h.5.ln_1 (LayerNorm)', '_module.transformer.h.5.attn.c_proj (Conv1D)', '_module.transformer.h.5.ln_2 (LayerNorm)', '_module.transformer.h.5.mlp.c_fc (Conv1D)', '_module.transformer.h.5.mlp.c_proj (Conv1D)', '_module.transformer.h.6.ln_1 (LayerNorm)', '_module.transformer.h.6...."]}]},{"cell_type":"code","source":["try:\n","    train_step = 0\n","    for epoch in itertools.count(start=1):\n","        train_step = train_validate(\n","            lm_net, optimizer, scheduler, train_loader, valid_loader, args,\n","            train_step=train_step, epoch=epoch\n","        )\n","\n","        # Printing epsilon from opacus privacy engine at the end of each epoch\n","        eps, alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n","        print(\"End of epoch {}, we have epsilon {} for alpha {}\".format(epoch, eps, alpha))\n","\n","        if train_step >= args.max_step or (args.max_epoch is not None and epoch >= args.max_epoch):\n","            if args.rank == 0:\n","                print('-' * 100)\n","                print('End of training')\n","            break\n","except KeyboardInterrupt:\n","    if args.rank == 0:\n","        print('-' * 100)\n","        print('Exiting from training early')\n","\n","distributed_sync(args)\n","print('cleanup dist ...')\n","cleanup(args)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":470},"id":"ET3Sp9PB6o_A","executionInfo":{"status":"error","timestamp":1704547904147,"user_tz":-210,"elapsed":596,"user":{"displayName":"Alireza Amiri","userId":"02703905429690679264"}},"outputId":"1aa7f5e0-44c2-445f-af3c-1a38c1207606"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["start to train the model................ 1\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 1.15 GiB (GPU 0; 15.77 GiB total capacity; 12.94 GiB already allocated; 356.38 MiB free; 14.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-63-3eec71b6d0fd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         train_step = train_validate(\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mlm_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-61-36de371e43ed>\u001b[0m in \u001b[0;36mtrain_validate\u001b[0;34m(model, optimizer, scheduler, train_loader, valid_loader, args, train_step, epoch)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m#     _lm_loss/(args.grad_acc), optimizer, model, scheduler, args, is_update=is_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         optimizer_step(\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0m_lm_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         )\n","\u001b[0;32m<ipython-input-61-36de371e43ed>\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(_loss, _optimizer, _model, _schedule, args, is_update)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m#        _scaled_loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0m_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_update\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opacus/grad_sample/grad_sample_module.py\u001b[0m in \u001b[0;36mcapture_backprops_hook\u001b[0;34m(self, module, _forward_input, forward_output, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    207\u001b[0m         )\n\u001b[1;32m    208\u001b[0m         \u001b[0mgrad_sampler_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRAD_SAMPLERS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mgrad_sampler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opacus/grad_sample/linear.py\u001b[0m in \u001b[0;36mcompute_linear_grad_sample\u001b[0;34m(layer, A, B, batch_dim)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \"\"\"\n\u001b[1;32m     23\u001b[0m     \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n...i,n...j->nij\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mcreate_or_extend_grad_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opacus/grad_sample/utils.py\u001b[0m in \u001b[0;36mcreate_or_extend_grad_sample\u001b[0;34m(param, grad_sample, batch_dim)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"grad_sample\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.15 GiB (GPU 0; 15.77 GiB total capacity; 12.94 GiB already allocated; 356.38 MiB free; 14.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]}]}